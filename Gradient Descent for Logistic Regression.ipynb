{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6eacd6a",
   "metadata": {
    "papermill": {
     "duration": 0.003645,
     "end_time": "2025-07-28T21:38:18.190373",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.186728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Logistic Gradient Descent\n",
    "\n",
    "Recall the gradient descent algorithm utilizes the gradient calculation:\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "Where each iteration performs simultaneous updates on $w_j$ for all $j$, where\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0366504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.198332Z",
     "iopub.status.busy": "2025-07-28T21:38:18.197372Z",
     "iopub.status.idle": "2025-07-28T21:38:18.206428Z",
     "shell.execute_reply": "2025-07-28T21:38:18.205696Z"
    },
    "papermill": {
     "duration": 0.014469,
     "end_time": "2025-07-28T21:38:18.208118",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.193649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d53f061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.215299Z",
     "iopub.status.busy": "2025-07-28T21:38:18.215019Z",
     "iopub.status.idle": "2025-07-28T21:38:18.219892Z",
     "shell.execute_reply": "2025-07-28T21:38:18.219110Z"
    },
    "papermill": {
     "duration": 0.010251,
     "end_time": "2025-07-28T21:38:18.221477",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.211226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc55259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.228826Z",
     "iopub.status.busy": "2025-07-28T21:38:18.228471Z",
     "iopub.status.idle": "2025-07-28T21:38:18.233235Z",
     "shell.execute_reply": "2025-07-28T21:38:18.232311Z"
    },
    "papermill": {
     "duration": 0.01058,
     "end_time": "2025-07-28T21:38:18.235097",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.224517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    exp_neg_x = math.exp(-x)\n",
    "    denominator = 1 + exp_neg_x\n",
    "    result = 1 / denominator\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b60c7",
   "metadata": {
    "papermill": {
     "duration": 0.002377,
     "end_time": "2025-07-28T21:38:18.240325",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.237948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Computing cost function for logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae3206c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.247010Z",
     "iopub.status.busy": "2025-07-28T21:38:18.246692Z",
     "iopub.status.idle": "2025-07-28T21:38:18.252703Z",
     "shell.execute_reply": "2025-07-28T21:38:18.251808Z"
    },
    "papermill": {
     "duration": 0.011542,
     "end_time": "2025-07-28T21:38:18.254411",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.242869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        z_i = np.dot(w,X[i]) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost += -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "    cost = cost / m\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ea94f",
   "metadata": {
    "papermill": {
     "duration": 0.002517,
     "end_time": "2025-07-28T21:38:18.259768",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.257251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Gradient Descent Implementation\n",
    "The gradient descent algorithm implementation has two components: \n",
    "- The loop implementing equation (1) above. This is `gradient_descent` below and is generally provided to you in optional and practice labs.\n",
    "- The calculation of the current gradient, equations (2,3) above. This is `compute_gradient_logistic` below.\n",
    "\n",
    "#### Calculating the Gradient, Code Description\n",
    "Implements equation (2),(3) above for all $w_j$ and $b$.\n",
    "There are many ways to implement this. Outlined below is this:\n",
    "- initialize variables to accumulate `dj_dw` and `dj_db`\n",
    "- for each example\n",
    "    - calculate the error for that example $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$\n",
    "    - for each input value $x_{j}^{(i)}$ in this example,  \n",
    "        - multiply the error by the input  $x_{j}^{(i)}$, and add to the corresponding element of `dj_dw`. (equation 2 above)\n",
    "    - add the error to `dj_db` (equation 3 above)\n",
    "\n",
    "- divide `dj_db` and `dj_dw` by total number of examples (m)\n",
    "- note that $\\mathbf{x}^{(i)}$ in numpy `X[i,:]` or `X[i]`  and $x_{j}^{(i)}$ is `X[i,j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e9a026",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.266155Z",
     "iopub.status.busy": "2025-07-28T21:38:18.265877Z",
     "iopub.status.idle": "2025-07-28T21:38:18.271641Z",
     "shell.execute_reply": "2025-07-28T21:38:18.270683Z"
    },
    "papermill": {
     "duration": 0.010865,
     "end_time": "2025-07-28T21:38:18.273181",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.262316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                         \n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          \n",
    "        err_i  = f_wb_i  - y[i]                      \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      \n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   \n",
    "    dj_db = dj_db/m                                   \n",
    "        \n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f1d23a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.280384Z",
     "iopub.status.busy": "2025-07-28T21:38:18.280115Z",
     "iopub.status.idle": "2025-07-28T21:38:18.290446Z",
     "shell.execute_reply": "2025-07-28T21:38:18.289319Z"
    },
    "papermill": {
     "duration": 0.016051,
     "end_time": "2025-07-28T21:38:18.292080",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.276029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.49861806546328574\n",
      "dj_dw: [0.498333393278696, 0.49883942983996693]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec935aa",
   "metadata": {
    "papermill": {
     "duration": 0.002565,
     "end_time": "2025-07-28T21:38:18.297429",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.294864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Gradient Descent Code \n",
    "The code implementing equation (1) above is implemented below. Take a moment to locate and compare the functions in the routine to the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a165d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.303811Z",
     "iopub.status.busy": "2025-07-28T21:38:18.303456Z",
     "iopub.status.idle": "2025-07-28T21:38:18.309805Z",
     "shell.execute_reply": "2025-07-28T21:38:18.308967Z"
    },
    "papermill": {
     "duration": 0.011365,
     "end_time": "2025-07-28T21:38:18.311348",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.299983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6bb490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T21:38:18.317925Z",
     "iopub.status.busy": "2025-07-28T21:38:18.317569Z",
     "iopub.status.idle": "2025-07-28T21:38:19.015018Z",
     "shell.execute_reply": "2025-07-28T21:38:19.013722Z"
    },
    "papermill": {
     "duration": 0.702647,
     "end_time": "2025-07-28T21:38:19.016747",
     "exception": false,
     "start_time": "2025-07-28T21:38:18.314100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.684610468560574   \n",
      "Iteration 1000: Cost 0.1590977666870457   \n",
      "Iteration 2000: Cost 0.08460064176930078   \n",
      "Iteration 3000: Cost 0.05705327279402531   \n",
      "Iteration 4000: Cost 0.04290759421682   \n",
      "Iteration 5000: Cost 0.03433847729884557   \n",
      "Iteration 6000: Cost 0.02860379802212006   \n",
      "Iteration 7000: Cost 0.02450156960879306   \n",
      "Iteration 8000: Cost 0.02142370332569295   \n",
      "Iteration 9000: Cost 0.019030137124109114   \n",
      "\n",
      "updated parameters: w:[5.28123029 5.07815608], b:-14.222409982019837\n"
     ]
    }
   ],
   "source": [
    "w_tmp  = np.zeros_like(X_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.321245,
   "end_time": "2025-07-28T21:38:19.439428",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-28T21:38:13.118183",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
