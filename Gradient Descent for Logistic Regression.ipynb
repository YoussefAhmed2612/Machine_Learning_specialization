{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Logistic Gradient Descent\n\nRecall the gradient descent algorithm utilizes the gradient calculation:\n$$\\begin{align*}\n&\\text{repeat until convergence:} \\; \\lbrace \\\\\n&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n&\\rbrace\n\\end{align*}$$\n\nWhere each iteration performs simultaneous updates on $w_j$ for all $j$, where\n$$\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n\\end{align*}$$\n\n","metadata":{}},{"cell_type":"code","source":"import copy, math\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.507909Z","iopub.execute_input":"2025-07-28T21:35:12.508233Z","iopub.status.idle":"2025-07-28T21:35:12.512930Z","shell.execute_reply.started":"2025-07-28T21:35:12.508211Z","shell.execute_reply":"2025-07-28T21:35:12.511823Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_train = np.array([0, 0, 0, 1, 1, 1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.514604Z","iopub.execute_input":"2025-07-28T21:35:12.514928Z","iopub.status.idle":"2025-07-28T21:35:12.536429Z","shell.execute_reply.started":"2025-07-28T21:35:12.514897Z","shell.execute_reply":"2025-07-28T21:35:12.535031Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def sigmoid(x):\n    exp_neg_x = math.exp(-x)\n    denominator = 1 + exp_neg_x\n    result = 1 / denominator\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.537510Z","iopub.execute_input":"2025-07-28T21:35:12.537743Z","iopub.status.idle":"2025-07-28T21:35:12.555288Z","shell.execute_reply.started":"2025-07-28T21:35:12.537726Z","shell.execute_reply":"2025-07-28T21:35:12.554398Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Computing cost function for logistic regression\n","metadata":{}},{"cell_type":"code","source":"def compute_cost_logistic(X, y, w, b):\n    \n    m = X.shape[0]\n    cost = 0.0\n    \n    for i in range(m):\n        z_i = np.dot(w,X[i]) + b\n        f_wb_i = sigmoid(z_i)\n        cost += -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n    cost = cost / m\n    \n    return cost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.557251Z","iopub.execute_input":"2025-07-28T21:35:12.557490Z","iopub.status.idle":"2025-07-28T21:35:12.574365Z","shell.execute_reply.started":"2025-07-28T21:35:12.557472Z","shell.execute_reply":"2025-07-28T21:35:12.573437Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Gradient Descent Implementation\nThe gradient descent algorithm implementation has two components: \n- The loop implementing equation (1) above. This is `gradient_descent` below and is generally provided to you in optional and practice labs.\n- The calculation of the current gradient, equations (2,3) above. This is `compute_gradient_logistic` below.\n\n#### Calculating the Gradient, Code Description\nImplements equation (2),(3) above for all $w_j$ and $b$.\nThere are many ways to implement this. Outlined below is this:\n- initialize variables to accumulate `dj_dw` and `dj_db`\n- for each example\n    - calculate the error for that example $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$\n    - for each input value $x_{j}^{(i)}$ in this example,  \n        - multiply the error by the input  $x_{j}^{(i)}$, and add to the corresponding element of `dj_dw`. (equation 2 above)\n    - add the error to `dj_db` (equation 3 above)\n\n- divide `dj_db` and `dj_dw` by total number of examples (m)\n- note that $\\mathbf{x}^{(i)}$ in numpy `X[i,:]` or `X[i]`  and $x_{j}^{(i)}$ is `X[i,j]`","metadata":{}},{"cell_type":"code","source":"def compute_gradient_logistic(X, y, w, b): \n    \n    m,n = X.shape\n    dj_dw = np.zeros((n,))                         \n    dj_db = 0.\n\n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          \n        err_i  = f_wb_i  - y[i]                      \n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      \n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   \n    dj_db = dj_db/m                                   \n        \n    return dj_db, dj_dw  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.575336Z","iopub.execute_input":"2025-07-28T21:35:12.575582Z","iopub.status.idle":"2025-07-28T21:35:12.594937Z","shell.execute_reply.started":"2025-07-28T21:35:12.575558Z","shell.execute_reply":"2025-07-28T21:35:12.594016Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\nX_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_tmp = np.array([0, 0, 0, 1, 1, 1])\nw_tmp = np.array([2.,3.])\nb_tmp = 1.\ndj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\nprint(f\"dj_db: {dj_db_tmp}\" )\nprint(f\"dj_dw: {dj_dw_tmp.tolist()}\" )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.595858Z","iopub.execute_input":"2025-07-28T21:35:12.596191Z","iopub.status.idle":"2025-07-28T21:35:12.616203Z","shell.execute_reply.started":"2025-07-28T21:35:12.596164Z","shell.execute_reply":"2025-07-28T21:35:12.615326Z"}},"outputs":[{"name":"stdout","text":"dj_db: 0.49861806546328574\ndj_dw: [0.498333393278696, 0.49883942983996693]\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"#### Gradient Descent Code \nThe code implementing equation (1) above is implemented below. Take a moment to locate and compare the functions in the routine to the equations above.","metadata":{}},{"cell_type":"code","source":"def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_iters):\n        # Calculate the gradient and update the parameters\n        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n      \n        # Save cost J at each iteration\n        if i<100000:      # prevent resource exhaustion \n            J_history.append( compute_cost_logistic(X, y, w, b) )\n\n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n        \n    return w, b, J_history         #return final w,b and J history for graphing\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.617029Z","iopub.execute_input":"2025-07-28T21:35:12.617307Z","iopub.status.idle":"2025-07-28T21:35:12.636748Z","shell.execute_reply.started":"2025-07-28T21:35:12.617287Z","shell.execute_reply":"2025-07-28T21:35:12.635806Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"w_tmp  = np.zeros_like(X_train[0])\nb_tmp  = 0.\nalph = 0.1\niters = 10000\n\nw_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \nprint(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T21:35:12.637831Z","iopub.execute_input":"2025-07-28T21:35:12.638149Z","iopub.status.idle":"2025-07-28T21:35:13.342650Z","shell.execute_reply.started":"2025-07-28T21:35:12.638122Z","shell.execute_reply":"2025-07-28T21:35:13.341889Z"}},"outputs":[{"name":"stdout","text":"Iteration    0: Cost 0.684610468560574   \nIteration 1000: Cost 0.1590977666870457   \nIteration 2000: Cost 0.08460064176930078   \nIteration 3000: Cost 0.05705327279402531   \nIteration 4000: Cost 0.04290759421682   \nIteration 5000: Cost 0.03433847729884557   \nIteration 6000: Cost 0.02860379802212006   \nIteration 7000: Cost 0.02450156960879306   \nIteration 8000: Cost 0.02142370332569295   \nIteration 9000: Cost 0.019030137124109114   \n\nupdated parameters: w:[5.28123029 5.07815608], b:-14.222409982019837\n","output_type":"stream"}],"execution_count":24}]}